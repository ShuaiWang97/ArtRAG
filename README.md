# ArtRAG
This repository contains the code for the paper titled "ArtRAG: Retrieval-Augmented Generation with Structured Context for Visual Art Understanding".

**ArtRAG** is a Retrieval-Augmented Generation (RAG) framework tailored for multimodal visual art undterstanding. It integrates large language models (LLMs) and vision-language processing to generate and evaluate art-related content.

---

## ğŸ“ Project Structure

```
ArtRAG_software/
â”‚
â”œâ”€â”€ lightrag/                   
â”‚   â”œâ”€â”€ base.py                  # Base functionality
â”‚   â”œâ”€â”€ clip_score.py            # CLIP-based scoring for images
â”‚   â”œâ”€â”€ generation_eval_utils.py # Utilities for evaluation
â”‚   â”œâ”€â”€ lightrag.py              # RAG implementation of lightrag
â”‚   â”œâ”€â”€ llm.py                   # LLM interaction wrapper
â”‚   â”œâ”€â”€ operate.py               # Core execution logic
â”‚   â”œâ”€â”€ prompt_*.py              # Prompt templates and generators
â”‚   â”œâ”€â”€ prunning.py              # Pruning logic for candidate outputs
â”‚   â”œâ”€â”€ shared_storage.py        # Shared memory/storage handler
â”‚   â”œâ”€â”€ storage.py               # Storage utilities
â”‚   â””â”€â”€ utils.py                 # General utilities
â”‚
â”œâ”€â”€ built_graph/                # Prebuilt knowledge/graph resources
â”‚   â””â”€â”€ All_gpt_4o_mini_clean    # Example graph data
â”‚
â”œâ”€â”€ scripts/                    # Evaluation and setup scripts
â”‚   â”œâ”€â”€ build_graph.py
â”‚   â”œâ”€â”€ clean_graph.py
â”‚   â”œâ”€â”€ inference_eval_artpedia.py
â”‚   â”œâ”€â”€ inference_eval.py
â”‚   â”œâ”€â”€ run_eval.sh
â”‚   â””â”€â”€ art_evaluation_data.csv
â”‚
â””â”€â”€ README.md                   # You're reading it!
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- `pip` or `conda`
- Access to OpenAI or HuggingFace API 

---

## ğŸ§  Core Concepts

This framework is designed around the concept of **retrieval-augmented generation** applied to **art and multimodal tasks**.

- **Prompting Strategies:** Configurable in `prompt_*.py`
- **Model Execution:** Controlled via `llm.py` and `operate.py`
- **Evaluation:** Scripts in `/scripts` help benchmark output against datasets like **Artpedia**

---

## ğŸ›  Example Usage

### Build a Knowledge Graph

```bash
python scripts/build_graph.py
```

### Run Inference

```bash
python scripts/inference_eval.py
```

### Evaluate Against Artpedia

```bash
python scripts/inference_eval_artpedia.py
```

Or use the shell script:

```bash
bash scripts/run_eval.sh
```

The code base is built on lightrag project. Thanks for the great work.
